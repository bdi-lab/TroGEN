# Beneath the Facade: Probing Safety Vulnerabilities in LLMs via Auto-Generated Jailbreak Prompts

This is the official implementation of the paper:

**Beneath the Facade: Probing Safety Vulnerabilities in LLMs via Auto-Generated Jailbreak Prompts**

Accepted to the Findings of the Association for Computational Linguistics: EMNLP (Findings of EMNLP), 2025.

All codes are written by Heehyeon Kim (heehyeon@kaist.ac.kr) and Kyeongryul Lee (klee0257@kaist.ac.kr).\
If you use this code, please cite our paper.

## License
Our codes are released under the CC BY-NC-SA 4.0 license.
